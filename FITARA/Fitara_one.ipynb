{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection as model_selection\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# from sklearn.utils import shuffle\n",
    "# import stop_words\n",
    "TRAIN_LABELS = 'participants/train/labels/labels.csv'\n",
    "TRAIN_TEXT = 'participants/train/extracted_data/extract_combined.csv'\n",
    "TEST_TEXT = 'participants/test/extracted_data/extract_combined.csv'\n",
    "TRAIN_ORG_TEXT = 'participants/get_only_ORG.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = pd.read_csv(TRAIN_LABELS, usecols=['document_name','is_fitara'])\n",
    "train_text_df = pd.read_csv(TRAIN_TEXT)\n",
    "test_df = pd.read_csv(TEST_TEXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>is_fitara</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04-42RFP.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n \\nOMB No. 0990-0115  \\n \\n Electronic Requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_-_Brand_name_JOFOC_1782798.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n \\nJustification for Other than Full and Op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_-_Brand_Name_JOFOC_for_FBO.docx</td>\n",
       "      <td>0</td>\n",
       "      <td>Justification for Other than Full and Open Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_-_Brand_Name_Only.docx</td>\n",
       "      <td>1</td>\n",
       "      <td>Justification for Other than Full and Open Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_-_JOFOC_-_FSSI_Limited_Source.doc</td>\n",
       "      <td>1</td>\n",
       "      <td>Attachment C\\r\\r\\rHHS Template and Instruction...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         document_name  is_fitara  \\\n",
       "0                         04-42RFP.pdf          0   \n",
       "1     1_-_Brand_name_JOFOC_1782798.pdf          0   \n",
       "2    1_-_Brand_Name_JOFOC_for_FBO.docx          0   \n",
       "3             1_-_Brand_Name_Only.docx          1   \n",
       "4  1_-_JOFOC_-_FSSI_Limited_Source.doc          1   \n",
       "\n",
       "                                                text  \n",
       "0   \\n \\nOMB No. 0990-0115  \\n \\n Electronic Requ...  \n",
       "1   \\n \\nJustification for Other than Full and Op...  \n",
       "2  Justification for Other than Full and Open Com...  \n",
       "3  Justification for Other than Full and Open Com...  \n",
       "4  Attachment C\\r\\r\\rHHS Template and Instruction...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_df.is_fitara = train_labels_df.is_fitara.eq('Yes').mul(1)\n",
    "\n",
    "train_df = pd.merge(\n",
    "    train_labels_df, \n",
    "    train_text_df, \n",
    "    on='document_name', \n",
    "    how='inner'\n",
    ")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)  # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)  # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing.\n",
    "    text = text.replace('x', '')\n",
    "    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)  # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(train_df.text, train_df.is_fitara, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer( max_df=0.7, analyzer='word', ngram_range=(2,3), token_pattern='\\w{1,}' ,max_features=50000)\n",
    "#stop_words='english',\n",
    "#, token_pattern=r'\\w{1,}'\n",
    "tfidf_vect_ngram.fit(train_x)\n",
    "ngram_train = tfidf_vect_ngram.transform(train_x.values)\n",
    "ngram_test = tfidf_vect_ngram.transform(test_x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 79.02, NNZs: 49737, Bias: -1.064832, T: 668, Avg. loss: 0.718223\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 68.03, NNZs: 49864, Bias: -1.320871, T: 1336, Avg. loss: 0.190823\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 58.95, NNZs: 49882, Bias: -1.212988, T: 2004, Avg. loss: 0.089696\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 53.24, NNZs: 49905, Bias: -1.332375, T: 2672, Avg. loss: 0.064695\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 49.03, NNZs: 49911, Bias: -1.229155, T: 3340, Avg. loss: 0.051757\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 44.92, NNZs: 49930, Bias: -1.184454, T: 4008, Avg. loss: 0.036661\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 42.83, NNZs: 49937, Bias: -1.147682, T: 4676, Avg. loss: 0.030618\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 40.28, NNZs: 49953, Bias: -1.147056, T: 5344, Avg. loss: 0.027815\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 38.31, NNZs: 49959, Bias: -1.129807, T: 6012, Avg. loss: 0.027369\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 37.95, NNZs: 49961, Bias: -1.114789, T: 6680, Avg. loss: 0.025710\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 37.02, NNZs: 49962, Bias: -1.051640, T: 7348, Avg. loss: 0.023756\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 36.12, NNZs: 49964, Bias: -1.051420, T: 8016, Avg. loss: 0.018439\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 35.03, NNZs: 49966, Bias: -1.008760, T: 8684, Avg. loss: 0.017040\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 34.06, NNZs: 49966, Bias: -1.018523, T: 9352, Avg. loss: 0.016099\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 33.59, NNZs: 49967, Bias: -0.990929, T: 10020, Avg. loss: 0.017928\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 33.27, NNZs: 49976, Bias: -1.000967, T: 10688, Avg. loss: 0.019721\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 32.99, NNZs: 49980, Bias: -0.968082, T: 11356, Avg. loss: 0.017726\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 32.51, NNZs: 49996, Bias: -1.000553, T: 12024, Avg. loss: 0.015942\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 31.83, NNZs: 49996, Bias: -1.000582, T: 12692, Avg. loss: 0.014217\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 31.89, NNZs: 49996, Bias: -0.979782, T: 13360, Avg. loss: 0.015049\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 31.74, NNZs: 49996, Bias: -1.001416, T: 14028, Avg. loss: 0.014947\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 31.78, NNZs: 49996, Bias: -0.988656, T: 14696, Avg. loss: 0.013791\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 31.46, NNZs: 49996, Bias: -0.982775, T: 15364, Avg. loss: 0.012979\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 30.85, NNZs: 49996, Bias: -1.018386, T: 16032, Avg. loss: 0.010540\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 30.44, NNZs: 49996, Bias: -1.006478, T: 16700, Avg. loss: 0.010634\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 30.27, NNZs: 49996, Bias: -0.973472, T: 17368, Avg. loss: 0.010490\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 30.24, NNZs: 49996, Bias: -1.011085, T: 18036, Avg. loss: 0.011888\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 30.19, NNZs: 49996, Bias: -0.985252, T: 18704, Avg. loss: 0.013061\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 30.05, NNZs: 49996, Bias: -0.985694, T: 19372, Avg. loss: 0.011253\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 29.80, NNZs: 49996, Bias: -1.000007, T: 20040, Avg. loss: 0.011993\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 29.64, NNZs: 49996, Bias: -0.986113, T: 20708, Avg. loss: 0.009512\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 29.70, NNZs: 49996, Bias: -0.986304, T: 21376, Avg. loss: 0.011803\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 29.49, NNZs: 49996, Bias: -0.994933, T: 22044, Avg. loss: 0.009689\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 29.43, NNZs: 49996, Bias: -0.995093, T: 22712, Avg. loss: 0.009949\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 29.32, NNZs: 49996, Bias: -1.003438, T: 23380, Avg. loss: 0.009327\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 29.07, NNZs: 49996, Bias: -0.999369, T: 24048, Avg. loss: 0.008592\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 29.03, NNZs: 49996, Bias: -1.003261, T: 24716, Avg. loss: 0.009607\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 29.07, NNZs: 49996, Bias: -1.003345, T: 25384, Avg. loss: 0.010600\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 28.99, NNZs: 49996, Bias: -0.992092, T: 26052, Avg. loss: 0.009030\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 28.89, NNZs: 49996, Bias: -0.992141, T: 26720, Avg. loss: 0.008475\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 28.80, NNZs: 49996, Bias: -0.992329, T: 27388, Avg. loss: 0.008780\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 28.77, NNZs: 49996, Bias: -1.002775, T: 28056, Avg. loss: 0.008840\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 28.79, NNZs: 49996, Bias: -0.999330, T: 28724, Avg. loss: 0.008113\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 28.70, NNZs: 49996, Bias: -0.999397, T: 29392, Avg. loss: 0.008376\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 28.63, NNZs: 49996, Bias: -1.009167, T: 30060, Avg. loss: 0.008591\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 28.52, NNZs: 49996, Bias: -0.996406, T: 30728, Avg. loss: 0.007357\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 28.44, NNZs: 49996, Bias: -1.008962, T: 31396, Avg. loss: 0.007456\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 28.37, NNZs: 49996, Bias: -1.005934, T: 32064, Avg. loss: 0.007633\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 28.35, NNZs: 49996, Bias: -1.008830, T: 32732, Avg. loss: 0.008805\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 28.21, NNZs: 49996, Bias: -1.008713, T: 33400, Avg. loss: 0.007880\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 28.17, NNZs: 49996, Bias: -1.000034, T: 34068, Avg. loss: 0.007937\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 28.18, NNZs: 49996, Bias: -0.991632, T: 34736, Avg. loss: 0.007733\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 28.19, NNZs: 49996, Bias: -0.994415, T: 35404, Avg. loss: 0.007811\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 28.13, NNZs: 49996, Bias: -0.997092, T: 36072, Avg. loss: 0.007603\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 28.09, NNZs: 49996, Bias: -0.991810, T: 36740, Avg. loss: 0.007789\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 28.08, NNZs: 49996, Bias: -1.004954, T: 37408, Avg. loss: 0.007668\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 27.96, NNZs: 49996, Bias: -1.007458, T: 38076, Avg. loss: 0.007684\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 27.99, NNZs: 49996, Bias: -0.997279, T: 38744, Avg. loss: 0.007927\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 27.82, NNZs: 49996, Bias: -0.992382, T: 39412, Avg. loss: 0.006548\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 27.76, NNZs: 49996, Bias: -0.997373, T: 40080, Avg. loss: 0.007380\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 27.79, NNZs: 49996, Bias: -1.002141, T: 40748, Avg. loss: 0.007607\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 27.75, NNZs: 49996, Bias: -1.002098, T: 41416, Avg. loss: 0.006755\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 27.87, NNZs: 49996, Bias: -0.999697, T: 42084, Avg. loss: 0.007392\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 27.86, NNZs: 49996, Bias: -1.001969, T: 42752, Avg. loss: 0.007589\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 27.78, NNZs: 49996, Bias: -0.999721, T: 43420, Avg. loss: 0.007000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 27.76, NNZs: 49996, Bias: -1.006392, T: 44088, Avg. loss: 0.006932\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 27.62, NNZs: 49996, Bias: -1.001974, T: 44756, Avg. loss: 0.006435\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 27.60, NNZs: 49996, Bias: -0.991200, T: 45424, Avg. loss: 0.006459\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 27.56, NNZs: 49996, Bias: -0.997667, T: 46092, Avg. loss: 0.006866\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 27.61, NNZs: 49996, Bias: -0.993490, T: 46760, Avg. loss: 0.006795\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 27.50, NNZs: 49996, Bias: -1.001828, T: 47428, Avg. loss: 0.006343\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 27.48, NNZs: 49996, Bias: -0.997726, T: 48096, Avg. loss: 0.006317\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 27.51, NNZs: 49996, Bias: -0.995719, T: 48764, Avg. loss: 0.007141\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 27.52, NNZs: 49996, Bias: -1.003658, T: 49432, Avg. loss: 0.006623\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 27.53, NNZs: 49996, Bias: -0.997742, T: 50100, Avg. loss: 0.007157\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 27.46, NNZs: 49996, Bias: -1.001638, T: 50768, Avg. loss: 0.005995\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 27.43, NNZs: 49996, Bias: -0.999696, T: 51436, Avg. loss: 0.006424\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 27.40, NNZs: 49996, Bias: -1.007234, T: 52104, Avg. loss: 0.006945\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 27.32, NNZs: 49996, Bias: -1.009049, T: 52772, Avg. loss: 0.006316\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 27.35, NNZs: 49996, Bias: -0.996130, T: 53440, Avg. loss: 0.006524\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 27.30, NNZs: 49996, Bias: -0.999832, T: 54108, Avg. loss: 0.006250\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 27.26, NNZs: 49996, Bias: -1.003436, T: 54776, Avg. loss: 0.006539\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 27.30, NNZs: 49996, Bias: -1.001645, T: 55444, Avg. loss: 0.006979\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 27.25, NNZs: 49996, Bias: -1.001645, T: 56112, Avg. loss: 0.006263\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 27.33, NNZs: 49996, Bias: -0.996434, T: 56780, Avg. loss: 0.006822\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 27.37, NNZs: 49996, Bias: -0.996458, T: 57448, Avg. loss: 0.006886\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 27.28, NNZs: 49996, Bias: -1.004931, T: 58116, Avg. loss: 0.005845\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 27.24, NNZs: 49996, Bias: -0.998192, T: 58784, Avg. loss: 0.006081\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 27.23, NNZs: 49996, Bias: -0.998183, T: 59452, Avg. loss: 0.006284\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 27.18, NNZs: 49996, Bias: -1.001455, T: 60120, Avg. loss: 0.006108\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 27.14, NNZs: 49996, Bias: -0.998217, T: 60788, Avg. loss: 0.005999\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 27.17, NNZs: 49996, Bias: -1.004632, T: 61456, Avg. loss: 0.006418\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 27.12, NNZs: 49996, Bias: -1.001431, T: 62124, Avg. loss: 0.006057\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 27.15, NNZs: 49996, Bias: -0.999845, T: 62792, Avg. loss: 0.006104\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 27.15, NNZs: 49996, Bias: -0.998312, T: 63460, Avg. loss: 0.005868\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 27.16, NNZs: 49996, Bias: -1.001389, T: 64128, Avg. loss: 0.005858\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 27.11, NNZs: 49996, Bias: -1.001385, T: 64796, Avg. loss: 0.005708\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 27.14, NNZs: 49996, Bias: -0.999866, T: 65464, Avg. loss: 0.006010\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 27.14, NNZs: 49996, Bias: -0.996900, T: 66132, Avg. loss: 0.005843\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 27.05, NNZs: 49996, Bias: -0.999863, T: 66800, Avg. loss: 0.005352\n",
      "Total training time: 0.43 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=100,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Loss 4.45\n",
    "nb_classifier2 = SGDClassifier(max_iter=100, verbose=1)#class_weight={ 0:0.3, 1:0.7 })\n",
    "# for i in range(3):\n",
    "nb_classifier2.fit(ngram_train, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 6.98\n",
    "# nb_classifier2 = MultinomialNB(class_prior=[.5, .5])\n",
    "# nb_classifier2.fit(ngram_train, train_y)\n",
    "# ngram_pred = nb_classifier2.predict(ngram_test)\n",
    "# ngram_score = metrics.accuracy_score(test_y, ngram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= NGRAM VECTORIZER ================\n",
      "Loss =  4.813819679956798\n",
      "Score =  0.8606271777003485\n",
      "CM = \n",
      " [[190  19]\n",
      " [ 21  57]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ngram_pred = nb_classifier2.predict(ngram_test)\n",
    "ngram_score = metrics.accuracy_score(test_y, ngram_pred)\n",
    "print('============= NGRAM VECTORIZER ================')\n",
    "print('Loss = ',metrics.log_loss(test_y, ngram_pred))\n",
    "print('Score = ',ngram_score)\n",
    "print('CM = \\n',metrics.confusion_matrix(test_y, ngram_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(train_df.text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norovirus Therapeutics ORG\n",
      "Vaccines Total ORG\n",
      "ETA ORG\n",
      "Noroviruses ORG\n",
      "KingFisher Flex Purification System ORG\n",
      "KingFisher Flex Purification System ORG\n",
      "Manufacturer is Life Technologies Corporation ORG\n",
      "Thermo Fisher Scientific ORG\n",
      "Grand ORG\n",
      "RNA ORG\n",
      "USC ORG\n",
      "FSS ORG\n",
      "NIH ORG\n",
      "Contracting Officer ORG\n",
      "USC ORG\n",
      "Unusual & ORG\n",
      "USC ORG\n",
      "USC ORG\n",
      "USC ORG\n",
      "USC ORG\n",
      "USC ORG\n",
      "Life Technologies ORG\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for ent in doc.ents:\n",
    "    labels.add(ent.label_)\n",
    "    if ent.label_ == 'ORG':\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(train_df.text[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d7fee393a90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ORG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc2' is not defined"
     ]
    }
   ],
   "source": [
    "for ent in doc2.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "def get_only_ORG(text):\n",
    "    global count\n",
    "    doc = nlp(text)\n",
    "    ORGS = \"\"\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'ORG':\n",
    "            ORGS += \"\\t\"\n",
    "            ORGS += ent.text\n",
    "    count += 1\n",
    "    print(count)\n",
    "    return ORGS\n",
    "\n",
    "# count = 0\n",
    "train_df['text'] = train_df['text'].apply(get_only_ORG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('participants/get_only_ORG.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
